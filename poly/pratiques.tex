%-*- coding: iso-latin-1 -*-
\label{chap:pratiques}

\paragraph{Notions :} visualisation de données, représentativité des données,
équité des algorithmes, confidentialité des données, anonymisation,
responsabilité.

\paragraph{Objectifs pédagogiques :} 
\begin{itemize}      
  \setlength{\itemsep}{3pt}
\item S'interroger sur la pertinence d'une analyse de données et la validité
  des conclusions qui en sont tirées.
\end{itemize}

La science des données n'est pas uniquement une discipline technique : comme
souvent en ingénierie, nous ne pouvons pas dissocier les calculs que nous
faisons de la question posée ni de leur utilisation. Ce chapitre n'a pas
vocation à être un cours d'éthique\footnote{L'éthique peut être
  définie comme l'étude de la justification d'ue action à partir de normes,
  règles juridiques ou déontologiques, valeurs morales, intuitions et
  traditions qui peuvent être multiples et contradictoires au sein d'une même
  socitété.}, mais à vous donner quelques points d'entrée pour vous amener à
vous poser des questions sur l'usage de la science des données, de
l'apprentissage automaique et de l'intelligence artificielle. Pour cette
raison, vous trouverez plus de liens externes qu'à l'habitude à travers le
texte de ce chapitre, pointant tant vers des publications scientifiques que des
blogs de vulgarisation ou des articles de presse grand public. N'hésitez pas à
poursuivre vos propres lectures sur le sujet.

Nous motiverons ce chapitre par deux citations : la première, attribuée à
Benjamin Disraeli par Mark Twain, ``\textit{There are three kinds of lies:
  lies, damned lies, and statistics}'', et la seconde, attribuée à George
Box, ``\textit{All models are wrong, but some are useful}''.

\section{Visualisation de données}
La façon dont vous choisissez de représenter vos données ou vos résultats a un
impact fort sur le message que vous essayez de faire passer. 

Mi-mai 2020, le
Department of Public Health de l'État de Géorgie (États-Unis d'Amérique) a
publié le diagramme en barres de la figure~\ref{fig:georgia_wtf_barplot}. Regardez bien l'axe des abscisses : le message vous semble-t-il le même quand
les dates sont ordonnées de manière chronologique, comme sur la figure~\ref{fig:georgia_fixed_barplot} ?

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pratiques/georgia_wtf_barplot}
    \caption{Première version du diagramme en barres.}
    \label{fig:georgia_wtf_barplot}
  \end{subfigure} \hfill
  \begin{subfigure}[t]{0.47\textwidth}
    \includegraphics[width=\textwidth]{figures/pratiques/georgia_fixed_barplot}  
    \caption{Deuxième version du diagramme en barres.}
    \label{fig:georgia_fixed_barplot}
  \end{subfigure}  
  \caption{Deux variantes du même diagramme en barres publiées par le
    Department of Public Health de l'État de Géorgie à propos du nombre de cas
    de CoVid19.}
  %\label{fig:georgia_barplot}
\end{figure}

Il est donc très important de vous assurez que vos graphiques soient lisibles et qu'ils traduisent clairement votre message sans déformer les données. La visualisation des données, ou \textit{dataviz}, est un champ d'études à part entière.  Nous nous contenterons ici de citer quelques principes parmi
les plus importants.


\subsection{Le choix des axes}
%https://callingbullshit.org/tools/tools_misleading_axes.html 
Le choix des échelles et intervalles d'un graphique a une influence sur son
interprétation.

Pour un diagramme en barres, ne pas faire commencer les axes à 0 peut
artificiellement gonfler les différences entre les différentes barres. Ainsi,
le diagramme de la figure~\ref{fig:bars_start_nonzero} indique que le modèle 4
est bien supérieur aux autres, tandis que celui de la
figure~\ref{fig:bars_start_zero} montre des performances très comparables entre
les différentes méthodes. (Dans ce cas précis, il serait de toute façon
souhaitable de répéter plusieurs fois l'entrainement et l'évaluation, par
exemple avec une validation croisée (que nous verrons
section~\ref{sec:crossval}) et de produire des barres d'erreurs.)
\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pratiques/bars_start_nonzero}
    \caption{Axe des ordonnées réduit.}
    \label{fig:bars_start_nonzero}
  \end{subfigure} \hfill
  \begin{subfigure}[t]{0.47\textwidth}
    \includegraphics[width=\textwidth]{figures/pratiques/bars_start_zero}  
    \caption{Axe des ordonnées allant de 0 à 1.}
    \label{fig:bars_start_zero}
  \end{subfigure}  
  \caption{Deux façons de présenter la comparaison des performances de 4 modèles.}
  %\label{fig:georgia_barplot}
\end{figure}

À l'inverse, il pourra être préférable pour un diagramme dont le but est non
pas de comparer les valeurs absolues de variables mais plutôt de présenter leur
évolution que l'axe des ordonnées ne commence pas à zéro. Ainsi, la figure~\ref{fig:line_start_zero} indique une température très stable, tandis que la figure~\ref{fig:line_start_nonzero} permet de mieux rendre compte des variations.
\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pratiques/line_start_zero}
    \caption{Axe des ordonnées partant de 0K.}
    \label{fig:line_start_zero}
  \end{subfigure} \hfill
  \begin{subfigure}[t]{0.47\textwidth}
    \includegraphics[width=\textwidth]{figures/pratiques/line_start_nonzero}  
    \caption{Axe des ordonnées réduit.}
    \label{fig:line_start_nonzero}
  \end{subfigure}  
  \caption{Deux façons de présenter l'évolution des températures moyenne de la table~\ref{tab:meteo_data}.}
  %\label{fig:georgia_barplot}
\end{figure}

\subsection{\textit{Proportional ink} ou principe de l'encre proportionnelle}
%https://callingbullshit.org/tools/tools_proportional_ink.html
De manière générale, il est recommandé, lorsque l'on utilise des surfaces pour
représenter des nombres (par exemple, les rectangles d'un diagramme en barres),
que ces surfaces soient d'aires proportionnelles aux nombres en question. On
retrouve d'ailleurs ici l'idée de commencer les barres d'un diagramme en
barres à 0.

Il faut cependant faire aussi attention à ce que les surfaces en question soient faciles à comparer visuellement. Un diagramme camembert est ainsi préférable à un graphique à bulles ; mais un diagramme en barres est généralement plus lisible qu'un diagramme camembert. La figure~\ref{fig:areas} l'illustre. Il s'agit d'une variante d'une \href{https://www.jstor.org/stable/2288400?seq=1#metadata\_info\_tab\_contents}{expérience menée au début des années 1980} et souvent considérée comme fondatrice en \textit{dataviz}.

Remarquez ici que le diagramme en barres serait encore plus lisible sans couleurs (elles n'apportent rien) et en ordonnant les catégories par proportion.
\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.20\textwidth}
    \includegraphics[width=\textwidth]{figures/pratiques/areas_bubbles}  
    \caption{Graphique à bulles.}
    \label{fig:areas_bubbles}
  \end{subfigure}  \hfill
  \begin{subfigure}[t]{0.33\textwidth}
    \includegraphics[width=\textwidth]{figures/pratiques/areas_pie}  
    \caption{Diagramme camembert.}
    \label{fig:areas_pie}
  \end{subfigure} \hfill
  \begin{subfigure}[t]{0.33\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pratiques/areas_bars}
    \caption{Diagramme en barres.}
    \label{fig:areas_bars}
  \end{subfigure} 
  \caption{Trois façons de représenter les proportions de 8 catégories. Quelle(s) représentation(s) permettent de les classer aisément par ordre croissant ?}
  \label{fig:areas}
\end{figure}

%\subsection{Attention aux résumés}
%https://python-graph-gallery.com/39-hidden-data-under-boxplot/ 


\subsection{Dyschromatopie}
Nous ne percevons pas les couleurs de la même façon. Une forte proportion de la population est atteinte d'une forme ou d'une autre de dyschromatopie, la plus fréquente étant la deutéranopie (incapacité de différencier rouge et vert). 

Pour assurer une accessibilité maximale, utilisez des échelles de couleurs adaptées. Il est difficile de s'adapter à \textit{toutes} les dyschromatopies ; néanmoins le cycle par défaut de \texttt{matplotlib} est supposé être relativement adapté. Pour des \textit{heatmaps}, favoriser les échelles de couleur \textit{viridis} ou \textit{cividis} (voir figure~\ref{fig:pca_plot}). Des outils comme \href{https://www.color-blindness.com/coblis-color-blindness-simulator/}{CBLIS} ou \href{https://www.funkify.org}{Funkify} vous permettent de simuler différentes dyschromatopies pour vérifier la lisibilité de vos graphiques.


Vous pouvez aussi augmenter la lisibilité de vos graphiques en utilisant des
indices supplémentaires (épaisseur de trait, hachures, forme des points,
ordonner les légendes dans le même ordre que les courbes, etc.) et en doublant
vos images d'une description textuelle alternative pour les personnes
non-voyantes.


\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.30\textwidth}
    \includegraphics[width=\textwidth]{figures/pratiques/pca_plot_magma}  
    \caption{Magma.}
    \label{fig:pca_plot_magma}
  \end{subfigure}  \hfill
  \begin{subfigure}[t]{0.30\textwidth}
    \includegraphics[width=\textwidth]{figures/pratiques/pca_plot_viridis}  
    \caption{Viridis.}
    \label{fig:pca_plot_viridis}
  \end{subfigure} \hfill
  \begin{subfigure}[t]{0.30\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pratiques/pca_plot_cividis}
    \caption{Cividis.}
    \label{fig:pca_plot_cividis}
  \end{subfigure} 
  \caption{Athlètes de la PC2, représentés selon deux composantes, et colorés en fonction de leur classement, selon trois échelles de couleur différentes.}
  \label{fig:pca_plot}
\end{figure}




\section{Équité des algorithmes}
Une question importante qui se pose constamment en science des données est
celle de la \textbf{reproduction des biais}. En effet, un modèle appris sur un
jeu de données peut facilement reproduire des biais de ce jeu de données,
qu'ils soient explicites ou implicites.

Un exemple qui revient souvent est celui d'un algorithme de ressources humaines
utilisé par Amazon. Le modèle avait tendance à rejeter les candidatures posées
par des femmes. En effet, il était entraîné sur des données internes à
l'entreprise, dont les recrutements étaient fortement biaisés en faveur des
hommes. Bien que le genre n'ait pas été une variable utilisée pour décrire les
candidatures, le modèle détectait dans le texte des CV des informations
corrélée dans le jeu d'entraînement au rejet d'une candidature mais qui
s'avéraient surtout traduire qu'elle était posée par une femme (éducation dans
un établissement non-mixte réservé aux femmes ; appartenance à une équipe de
sport féminin, etc.).

Ainsi, ce n'est pas parce qu'un modèle statistique est purement mathématique
qu'il est impartial ; en particulier, un modèle ne peut pas être de meilleure
qualité que son jeu d'entraînement. Il faut donc réfléchir à la
\textbf{représentativité} des données : peut-on bien considérer qu'il s'agit
d'un échantillon aléatoire de la population qui nous intéresse, où ne
correspondent-elles qu'à une sous-population spécifique ?

Un autre exemple de reproduction des biais apparait dans une publication de
2016 qui présente un classifieur capable de distinguer criminels de
non-criminels à partir de simples photos. Cependant, les clichés de criminels
étaient des photos administratives prises de face, sans sourire, tandis que les
photos de non-criminels étaient des clichés plus flatteurs : le modèle
\href{https://callingbullshit.org/case\_studies/case\_study\_criminal\_machine\_learning.html}{détectait
  en fait les sourires}. On retrouve très souvent ce type d'erreurs, dûes à un
\textbf{facteur confondant} : on croit arriver à séparer des images sur leur
contenu alors qu'on utilise principalement leur luminosité ; ou à trouver des
facteurs génétiques influençant le niveau économique, alors que celui-ci est
fortement corrélé dans les données à la couleur de peau ; et ainsi de suite.

La question de l'équité des algorithmes est un sous-domaine important de
l'apprentissage automatique, et se pose d'autant plus que ses applications
s'étendent à des domaines divers et variés touchant de nombreux aspects de nos
sociétés : recruement mais aussi sécurité, santé, justice, etc. 
C'est le sujet par exemple de l'organisation
\href{https://www.fatml.org/}{Fairness, Accountability and Transparency in
  Machine Learning}.

Pour autant, il n'y a pas actuellement (et il n'y aura vraisemblablement
jamais) d'outils ou de procédures permettant de garantir cette équité. Il est
ainsi nécessaire de comprendre l'origine possible des biais, ainsi que de
développer des outils pour les mesurer.

Ces dernières années ont cependant vu l'émergence de labels, tels que le
\href{http://fdu-label.com/fr/index.html}{Fair Data Use} en France ou
\href{http://aequitas.dssg.io/}{Aequitas} aux USA, proposant une évaluation
éthique des outils numériques.

\section{Fiabilité}
Du diagnostic automatisé aux véhicules autonomes, nous avons de plus en plus
envie de faire confiance à l'intelligence artificielle pour les opportunités
qu'elle présente.

Mais comment faire confiance aux modèles et algorithmes issus de la science des
données ? Plusieurs questions se posent en plus de celle de l'équité discutée plus haut.

\paragraph{Vérifiabilité} les systèmes d'IA ont-ils le comportement attendu ?
Les \href{https://fr.wikipedia.org/wiki/M\%C3\%A9thode\_formelle\_(informatique)}{méthodes
  formelles}
typiquement utilisées en informatique pour les programmes utilisés en avionique
ne se prêtent guère aux modèles de l'apprentissage automatique, même si \href{https://formal-paris-saclay.fr/}{de
récents travaux émergent sur le sujet}.

\paragraph{Explicabilité et interprétabilité} Il s'agit aussi de vastes champs
d'étude. Si une régression linéaire est relativement interprétable (cf. PC 3),
des modèles paramétriques plus complexes tels que ceux produits par des réseaux
de neurones artificiels (voir chapitre~\ref{chap:nonlin}) le
sont beaucoup moins. 

\paragraph{Spécification} La description précise du comportement attendu
peut-elle aussi être délicate : quel choix doit faire un véhicule autonome
entre renverser une fillette et emboutir une moto avec deux passagers ? Le MIT
Media Lab propose par exemple \href{http://moralmachine.mit.edu/hl/fr}{La
  Machine Morale}, une plateforme permettant d'explorer divers dilemnes moraux
posés par la prise de décision de machines intelligentes.

\paragraph{Robustesse} Les modèles sont-ils robustes aux attaques ? Depuis
2015, les exemples montrant qu'il est possible d'induire facilement en erreur
un modèle appris par apprentissage automatique s'accumulent. Ces exemples
incluent l'ajout de bruit
\footnote{https://arxiv.org/abs/1412.6572}{indétectable à l'\oe{}il} ou
\href{https://nicholas.carlini.com/code/audio\_adversarial\_examples}{à
  l'oreille}, la \href{https://arxiv.org/abs/1710.08864}{modification d'un seul
  pixel} d'une image, ou
l'\href{https://towardsdatascience.com/poisoning-attacks-on-machine-learning-1ff247c254db}{empoisonnement}
d'un jeu de données, qui consiste à introduire au moment de l'apprentissage un
faible nombre d'exemples mal étiquetés ou ingénieusement calibrés pour induire
un comportement indésirable.

De même qu'en cryptographie où de nouveaux protocoles émergent pour faire face
à de nouvelles attaques de hackers, l'apprentissage automatique progresse aussi
pour répondre aux attaques
adversariales. \href{http://proceedings.mlr.press/v97/simon-gabriel19a.html}{De
  récents travaux} montrent même qu'en raison du fléau de la dimension, les
attaques adversariales sont inévitables en grande dimension.

\paragraph{Reproductibilité} La démarche scientifique repose sur la
reproductibilité des expériences. Au problème du \textit{p-hacking} abordé au
Chapitre~\ref{chap:tests} s'ajoute celui de la disponibilité des données, qui
peut être limitée pour des raisons de confidentialité, ainsi que la question
des \textbf{ressources informatiques} qui peuvent être nécessaires à entraîner
certains modèles. Reproduire des résultats obtenuse en faisant tourner 800
processeurs graphiques (GPUs) pendant 3 semaines nécessite des ressources
financières importantes (on rejoint ici des questions de coût énergétique et
écologique abordées dans la section~\ref{sec:ecology}).


\paragraph{Responsabilité} Qui est responsable en cas de faillite d'un
système d'IA : l'IA est-elle responsable ? Ou bien la personne qui l'utilise ?
Ou encore celle qui l'a construite ? La question s'est par exemple posée
lorsqu'un véhicule autonome
\href{https://www.nextinpact.com/news/108432-cause-probable-accident-mortel-uber-tout-monde-en-prend-pour-son-grade.htm}{a
  fauché une piétonne} en mars 2018.



\section{Confidentialité des données}
Une grande partie des données utilisées en science des données sont des données
personnelles, c'est-à-dire que les individus qu'elles décrivent sont des
personnes. Nombre d'entre nous s'inquiètent de ce que les données qui nous
concernent, qu'elles soient médicales, de localisation géographique, ou
concernent notre activité numérique, soient utilisées à bon escient.

Les \href{https://risques-tracage.fr/}{discussions autour des applications de
  traçage de contacts} dans la lutte contre la propagation du coronavirus vont
actuellement bon train, illustrant cette préoccupation.


En tant que \textit{data scientists}, comment nous assurer que nous ne
compromettons pas la confidentialité des personnes dont nous manipulons les
données ? Deux types de solutions techniques sont possibles.
\paragraph{Dé-identification algorithmique} Il s'agit de s'assurer que l'on ne
puisse pas remonter des données aux individus. Parmi ces techniques,
l'\textbf{anonymisation} consiste à supprimer suffisamment d'informations
identifiantes pour empêcher la réidentification. Ces informations peuvent être
\textbf{directement identifiantes} s'il s'agit de caractéristiques personnelles
uniques (nom, numéro de sécurité sociale, numéro de téléphone, etc.) ou
\textbf{indirectement identifiantes} si elles permettent d'identifier la
personne de manière unique quand elles sont croisées avec d'autres données (code postal, date de naissance et lieu de travail pris ensemble
peuvent être indirectement identifiants).  Par contraste, la
\textbf{confidentialité différentielle}, ou \textit{differential privacy} en
anglais cherche plutôt à garantir que les résultats d'une analyse sur une base de
données soient presque identiques qu'un échantillon soit présent ou non.

\paragraph{Sécurité des bases de données} Cet aspect inclut par exemple le
chiffrement homomorphique permettant d'obtenir les mêmes résultats sur données
chiffrées que non chiffrées, ne laissant ainsi aux \textit{data scientists} que
l'accès aux données chiffrées, des solutions de calcul distribué sécurisées, ou
encore du matériel cryptographique permettant d'exécuter du code sans que les
données ne soient visibles.

En France, la \href{https://www.cnil.fr/}{Commission Nationale de
  l'Informatique et des Libertés (CNIL)} encadre l'utilisation des données
personnelles, qui est notamment encadré par la loi du 14 mai 2018 transposant
le
\href{https://fr.wikipedia.org/wiki/R\%C3\%A8glement\_g\%C3\%A9n\%C3\%A9ral\_sur\_la\_protection\_des\_donn\%C3\%A9es}{Règlement
  Général sur la Protection des Données (RGPD)} de l'Union Européenne.


\section{Enjeux écologiques}
\label{sec:ecology}
\href{https://www.ademe.fr/sites/default/files/assets/documents/guide-pratique-face-cachee-numerique.pdf}{Selon
  l'ADEME}, le secteur du numérique est responsable de 4\% des émissions
mondiales de gaz à effet de serre, dont un quart dûs aux data
centers. Entraîner un réseau de neurones artificiels avec 213 millions de
paramètres peut générer \href{https://arxiv.org/abs/1906.02243}{autant
  d'émissions de CO2 que cinq voitures américaines} pendant toute leur
existence, fabrication comprise.  Le
\href{https://mlco2.github.io/impact/}{Machine Learning Emissions Calculator}
est un des outils qui accompagnent la prise de conscience de l'impact
environnemental de la science des données.


\begin{plusloin}
\item Des ouvrages entiers ont étés écrits sur la \textit{dataviz}, par exemple \href{https://serialmentor.com/dataviz/}{\textit{Fundamentals of Data Vizualization} de Claus O. Wilke}, le travail d'\href{https://www.edwardtufte.com/tufte/}{Edward Tufte}, ou encore \href{https://informationisbeautiful.net/}{\textit{Information is Beautiful} by David McCandless}.
\item \href{https://hippocrate.tech/}{Le Serment d'Hippocrate pour Data Scientist} de Data for Good.
\item La question de la représentativité se pose dans de nombreux
  domaines de l'ingénierie. Les exemples sont nombreux, des
  \href{https://www.huffingtonpost.fr/2017/08/19/ce-distributeur-automatique-ne-distribue-pas-de-savon-aux-mains\_a\_23152387/}{distributeurs
    de savon qui ne détectent que les peaux claires} à tous les
  objets plutôt adaptés aux hommes recensés par Caroline Criado
  Perez dans
  \href{https://www.liberation.fr/france/2020/03/06/les-femmes-invisibles-dans-un-monde-cree-pour-les-hommes\_1780895}{\textit{Invisible
      Women}}.
  \item Un épisode de La Méthode Scientifique  intitulé \href{https://april.org/ethique-numerique-des-datas-sous-serment-emission-la-methode-scientifique}{\textit{Éthique numérique, des data sous
    serment}}.
  \item {Fairness and Machine Learning} by Solon
    Barocas, Moritz Hardt and Arvind Narayanan.
  \item À propos de \href{https://www.latribune.fr/supplement/ceux-qui-transforment-la-france/la-justice-predictive-nouvel-outil-pour-les-professionnels-du-droit-837752.html}{justice prédictive}, l'article \href{https://www.dalloz-actualite.fr/flash/justice-et-intelligence-artificielle-preparer-demain-episode-i#.XsvEykNS8Xc}{Justice
        et intelligence artificielle : préparer demain}.
  \item \href{https://hbr.org/2013/04/the-hidden-biases-in-big-data}{\textit{The Hidden Biases in Big Data}}, Kate Crawford, HBR, April 2013. 
  \item \href{https://salil.seas.harvard.edu/files/salil/files/differential_privacy_primer_nontechnical_audience.pdf}{\textit{Differential privacy: A primer for a non-technical audience}}, A. Wood et al., Vanderbilt Journal of Entertainment and Technology Law.
\end{plusloin}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "sdd_2020_poly"
%%% End:

